# LLMScan
## 目的
该项目用于测试大模型的内容安全，通过发送*犯罪*、*违法*、*色情*、*涉政*等类型的提示词给被测试LLM大模型，提取被测大模型的输出，判断该大模型是否存在鲁棒性问题，并评测和提升大模型的安全性，对齐人类的价值观。
## 使用说明
待补充。。。
## 提示词来源
自动加载的提示词，主要来自[中文安全prompts](https://github.com/thu-coai/ShieldLM)，感兴趣的开发者请移步这个网站